# soft
## Vision Large Language Models Are Good Noise Handlers in Engagement Analysis
- **Url**: http://arxiv.org/abs/2511.14749v1
- **Authors**: ['Alexander Vedernikov', 'Puneet Kumar', 'Haoyu Chen', 'Tapio Seppänen', 'Xiaobai Li']
- **Abstrat**: Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06.





# robot
## $π^{*}_{0.6}$: a VLA That Learns From Experience
- **Url**: http://arxiv.org/abs/2511.14759v1
- **Authors**: ['Ali Amin', 'Raichelle Aniceto', 'Ashwin Balakrishna', 'Kevin Black', 'Ken Conley', 'Grace Connors', 'James Darpinian', 'Karan Dhabalia', 'Jared DiCarlo', 'Danny Driess', 'Michael Equi', 'Adnan Esmail', 'Yunhao Fang', 'Chelsea Finn', 'Catherine Glossop', 'Thomas Godden', 'Ivan Goryachev', 'Lachy Groom', 'Hunter Hancock', 'Karol Hausman', 'Gashon Hussein', 'Brian Ichter', 'Szymon Jakubczak', 'Rowan Jen', 'Tim Jones', 'Ben Katz', 'Liyiming Ke', 'Chandra Kuchi', 'Marinda Lamb', 'Devin LeBlanc', 'Sergey Levine', 'Adrian Li-Bell', 'Yao Lu', 'Vishnu Mano', 'Mohith Mothukuri', 'Suraj Nair', 'Karl Pertsch', 'Allen Z. Ren', 'Charvi Sharma', 'Lucy Xiaoyang Shi', 'Laura Smith', 'Jost Tobias Springenberg', 'Kyle Stachowicz', 'Will Stoeckle', 'Alex Swerdlow', 'James Tanner', 'Marcel Torne', 'Quan Vuong', 'Anna Walling', 'Haohuan Wang', 'Blake Williams', 'Sukwon Yoo', 'Lili Yu', 'Ury Zhilinsky', 'Zhiyuan Zhou']
- **Abstrat**: We study how vision-language-action (VLA) models can improve through real-world deployments via reinforcement learning (RL). We present a general-purpose method, RL with Experience and Corrections via Advantage-conditioned Policies (RECAP), that provides for RL training of VLAs via advantage conditioning. Our method incorporates heterogeneous data into the self-improvement process, including demonstrations, data from on-policy collection, and expert teleoperated interventions provided during autonomous execution. RECAP starts by pre-training a generalist VLA with offline RL, which we call $π^{*}_{0.6}$, that can then be specialized to attain high performance on downstream tasks through on-robot data collection. We show that the $π^{*}_{0.6}$ model trained with the full RECAP method can fold laundry in real homes, reliably assemble boxes, and make espresso drinks using a professional espresso machine. On some of the hardest tasks, RECAP more than doubles task throughput and roughly halves the task failure rate.





## HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation
- **Url**: http://arxiv.org/abs/2511.14756v1
- **Authors**: ['Lai Wei', 'Xuanbin Peng', 'Ri-Zhao Qiu', 'Tianshu Huang', 'Xuxin Cheng', 'Xiaolong Wang']
- **Abstrat**: Learning from real-world robot demonstrations holds promise for interacting with complex real-world environments. However, the complexity and variability of interaction dynamics often cause purely positional controllers to struggle with contacts or varying payloads. To address this, we propose a Heterogeneous Meta-Control (HMC) framework for Loco-Manipulation that adaptively stitches multiple control modalities: position, impedance, and hybrid force-position. We first introduce an interface, HMC-Controller, for blending actions from different control profiles continuously in the torque space. HMC-Controller facilitates both teleoperation and policy deployment. Then, to learn a robust force-aware policy, we propose HMC-Policy to unify different controllers into a heterogeneous architecture. We adopt a mixture-of-experts style routing to learn from large-scale position-only data and fine-grained force-aware demonstrations. Experiments on a real humanoid robot show over 50% relative improvement vs. baselines on challenging tasks such as compliant table wiping and drawer opening, demonstrating the efficacy of HMC.





## OG-VLA: Orthographic Image Generation for 3D-Aware Vision-Language Action Model
- **Url**: http://arxiv.org/abs/2506.01196v2
- **Authors**: ['Ishika Singh', 'Ankit Goyal', 'Stan Birchfield', 'Dieter Fox', 'Animesh Garg', 'Valts Blukis']
- **Abstrat**: We introduce OG-VLA, a novel architecture and learning framework that combines the generalization strengths of Vision Language Action models (VLAs) with the robustness of 3D-aware policies. We address the challenge of mapping natural language instructions and one or more RGBD observations to quasi-static robot actions. 3D-aware robot policies achieve state-of-the-art performance on precise robot manipulation tasks, but struggle with generalization to unseen instructions, scenes, and objects. On the other hand, VLAs excel at generalizing across instructions and scenes, but can be sensitive to camera and robot pose variations. We leverage prior knowledge embedded in language and vision foundation models to improve generalization of 3D-aware keyframe policies. OG-VLA unprojects input observations from diverse views into a point cloud which is then rendered from canonical orthographic views, ensuring input view invariance and consistency between input and output spaces. These canonical views are processed with a vision backbone, a Large Language Model (LLM), and an image diffusion model to generate images that encode the next position and orientation of the end-effector on the input scene. Evaluations on the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization to unseen environments, with over 40% relative improvements while maintaining robust performance in seen settings. We also show real-world adaption in 3 to 5 demonstrations along with strong generalization. Videos and resources at https://og-vla.github.io/




