# soft
## Multi-Modal Semantic Communication
- **Url**: http://arxiv.org/abs/2512.15691v1
- **Authors**: ['Matin Mortaheb', 'Erciyes Karakaya', 'Sennur Ulukus']
- **Abstrat**: Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.





## Divergences in the hadronic light-by-light amplitude of the holographic soft-wall model
- **Url**: http://arxiv.org/abs/2511.11797v2
- **Authors**: ['Josef Leutgeb', 'Jonas Mager', 'Anton Rebhan']
- **Abstrat**: We use the WKB approximation to uncover divergences and instances of non-commuting limits in a large class of holographic soft-wall models. We show that the infinite sum over single resonance contributions for a variety of observables involving the Chern-Simons term, such as the VVA correlator or the hadronic light-by-light tensor, does not converge. These divergences can in some cases (such as the VVA correlator) be traced back to non-commuting limits and avoided by working directly in the 5-dimensional setup with bulk-to-boundary propagators. However, the hadronic light-by-light scattering tensor also diverges in the 5-dimensional formulation with corresponding Green functions, preventing a correct implementation of the Melnikov-Vainshtein short-distance constraint and even leading to an infinite contribution to the muon $g-2$. We also discuss modifications of the standard soft-wall model that are able to resolve this issue.





# robot
## In Pursuit of Pixel Supervision for Visual Pre-training
- **Url**: http://arxiv.org/abs/2512.15715v1
- **Authors**: ['Lihe Yang', 'Shang-Wen Li', 'Yang Li', 'Xinjie Lei', 'Dong Wang', 'Abdelrahman Mohamed', 'Hengshuang Zhao', 'Hu Xu']
- **Abstrat**: At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed "Pixio", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.





## mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs
- **Url**: http://arxiv.org/abs/2512.15692v1
- **Authors**: ['Jonas Pai', 'Liam Achenbach', 'Victoriano Montesinos', 'Benedek Forrai', 'Oier Mees', 'Elvis Nava']
- **Abstrat**: Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.




