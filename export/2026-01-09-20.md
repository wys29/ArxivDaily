# soft
## Addressing Known Challenges in Solar Flare Forecasting I: Limb-Flare Prediction with a 4-pi Full-Heliosphere Framework
- **Url**: http://arxiv.org/abs/2601.05209v1
- **Authors**: ['K. D. Leka', 'Eric L. Wagner', 'Lisa Upton', 'Bibhuti Kumar Jha', 'Kiran Jain', 'Sara Petty']
- **Abstrat**: A demonstrated failure mode for operational solar flare forecasting is the inability to forecast flares that occur near, or just beyond, the solar limb. To address this shortcoming, we develop a "4pi" full-heliosphere event forecasting framework and evaluate its statistical classification ability against this specific challenge. A magnetic surface flux transport model is used to generate full-sun maps of the photospheric radial magnetic field from which active regions (ARs) are identified and tracked using a new labeling scheme that is observer-location agnostic and allows for post-facto modifications. Flare-relevant magnetic parameters couple to a "visibility" index that specifies AR location relative to the visible solar limb and expected flare detection. Flare labels are assigned according to peak Soft X-ray flux, and a statistical classification is performed using nonparametric discriminant analysis. A version where new or emerging ARs on the far ("invisible" side of the Sun are incorporated into the model by way of far-side helioseismology, is also tested. We evaluate the new framework by its performance specifically including the limb areas using Brier Skill Score and ROC Skill Score, finding improvement at the 2-sigma level or less. However, we do find that the number of False Negatives, or "missed" forecasts decreases, and find strong evidence that the additional information provided by the far-side helioseismology can help predict near- and just-beyond-limb flares, particularly for East-limb events. While individual components of this framework could be improved, we demonstrate that a known failure mode for solar flare forecasting can be mitigated with available resources.





## Control of the MoTe$_2$ Fermi Surface by Nb Doping
- **Url**: http://arxiv.org/abs/2601.05197v1
- **Authors**: ['Andrew P. Weber', 'Iñigo Robredo', 'Philipp Rüssmann', 'Maxim Ilyn', 'Arnaud Magrez', 'Philippe Bugnon', 'Nan Xu', 'Vladimir Strocov', 'J. Hugo Dil', 'J. Enrique Ortega', 'Julen Ibañez-Azpiroz']
- **Abstrat**: Ab initio calculations and angle-resolved photoemission experiments show that the bulk and surface electronic structure of Weyl semimetal candidate MoTe$_2$ changes significantly by tuning the chemical potential by less than 0.4 eV. Calculations show that several Lifshitz transitions can occur among multiple electron and hole Fermi pockets of differing orbital character. Experiments show that 18% Nb-Mo substitution reduces the occupation of bulk and (001) surface bands, effectively producing a chemical potential shift of $\approx 0.3$ eV. Orbital character and dimensionality of the bulk bands is examined by soft X-ray angle resolved photoemission with control of the excitation light polarization. The band filling at the surface is shown to increase upon deposition of alkali atoms. The results indicate that multiple regimes of electronic properties can be easily accessed in this versatile, layered material.





# robot
## LaST$_{0}$: Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model
- **Url**: http://arxiv.org/abs/2601.05248v1
- **Authors**: ['Zhuoyang Liu', 'Jiaming Liu', 'Hao Chen', 'Ziyu Guo', 'Chengkai Hou', 'Chenyang Gu', 'Jiale Yu', 'Xiangju Mi', 'Renrui Zhang', 'Zhengping Che', 'Jian Tang', 'Pheng-Ann Heng', 'Shanghang Zhang']
- **Abstrat**: Vision-Language-Action (VLA) models have recently demonstrated strong generalization capabilities in robotic manipulation. Some existing VLA approaches attempt to improve action accuracy by explicitly generating linguistic reasoning traces or future visual observations before action execution. However, explicit reasoning typically incurs non-negligible inference latency, which constrains the temporal resolution required for robotic manipulation. Moreover, such reasoning is confined to the linguistic space, imposing a representational bottleneck that struggles to faithfully capture ineffable physical attributes. To mitigate these limitations, we propose LaST$_0$, a framework that enables efficient reasoning before acting through a Latent Spatio-Temporal Chain-of-Thought (CoT), capturing fine-grained physical and robotic dynamics that are often difficult to verbalize. Specifically, we introduce a token-efficient latent CoT space that models future visual dynamics, 3D structural information, and robot proprioceptive states, and further extends these representations across time to enable temporally consistent implicit reasoning trajectories. Furthermore, LaST$_0$ adopts a dual-system architecture implemented via a Mixture-of-Transformers design, where a reasoning expert conducts low-frequency latent inference and an acting expert generates high-frequency actions conditioned on robotics-oriented latent representations. To facilitate coordination, LaST$_0$ is trained with heterogeneous operation frequencies, enabling adaptive switching between reasoning and action inference rates during deployment. Across ten simulated and six real-world manipulation tasks, LaST$_0$ improves mean success rates by 8% and 13% over prior VLA methods, respectively, while achieving substantially faster inference. Project website: https://sites.google.com/view/last0





## Pixel-Perfect Visual Geometry Estimation
- **Url**: http://arxiv.org/abs/2601.05246v1
- **Authors**: ['Gangwei Xu', 'Haotong Lin', 'Hongcheng Luo', 'Haiyang Sun', 'Bing Wang', 'Guang Chen', 'Sida Peng', 'Hangjun Ye', 'Xin Yang']
- **Abstrat**: Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.





## Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration
- **Url**: http://arxiv.org/abs/2601.05243v1
- **Authors**: ['Xingyi He', 'Adhitya Polavaram', 'Yunhao Cao', 'Om Deshmukh', 'Tianrui Wang', 'Xiaowei Zhou', 'Kuan Fang']
- **Abstrat**: Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.





## RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation
- **Url**: http://arxiv.org/abs/2601.05241v1
- **Authors**: ['Boyang Wang', 'Haoran Zhang', 'Shujie Zhang', 'Jinkun Hao', 'Mingda Jia', 'Qi Lv', 'Yucheng Mao', 'Zhaoyang Lyu', 'Jia Zeng', 'Xudong Xu', 'Jiangmiao Pang']
- **Abstrat**: The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.





## Plenoptic Video Generation
- **Url**: http://arxiv.org/abs/2601.05239v1
- **Authors**: ['Xiao Fu', 'Shitao Tang', 'Min Shi', 'Xian Liu', 'Jinwei Gu', 'Ming-Yu Liu', 'Dahua Lin', 'Chen-Hsuan Lin']
- **Abstrat**: Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/





## Learning Latent Action World Models In The Wild
- **Url**: http://arxiv.org/abs/2601.05230v1
- **Authors**: ['Quentin Garrido', 'Tushar Nagarajan', 'Basile Terver', 'Nicolas Ballas', 'Yann LeCun', 'Michael Rabbat']
- **Abstrat**: Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.




