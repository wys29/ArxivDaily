# soft
## Bridging the Gap Between Avalanche Relaxation and Yielding Rheology
- **Url**: http://arxiv.org/abs/2504.18382v2
- **Authors**: ['Leonardo Relmucao-Leiva', 'Carlos Villarroel', 'Gustavo DÃ¼ring']
- **Abstrat**: The yielding transition in amorphous materials, whether driven passively (simple shear) or actively, remains a fundamental open question in soft matter physics. While avalanche statistics at the critical point have been extensively studied, the emergence of the dynamic regime at yielding and the steady-state flow properties remain poorly understood. In particular, the significant variability observed in flow curves across different systems lacks a clear explanation. We determine, for the first time, the relationship between avalanche duration and size across the yielding transition, revealing how it evolves from quasistatic to dynamic flow regimes. This precise measurement is made using the Controlled Relaxation Time Model (CRTM), a new simulation framework that treats the relaxation time as a tunable parameter. CRTM reproduces known results in both limits and enables a direct analysis of the change of regime between them. Applying the model to different microscopic dynamics, we find that the existing scaling relation connecting critical exponents under flow holds for passive systems. However, active systems exhibit significant deviations, suggesting a missing ingredient in the current understanding of yielding.





# robot
## Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision
- **Url**: http://arxiv.org/abs/2512.10956v1
- **Authors**: ['Wentao Zhou', 'Xuweiyi Chen', 'Vignesh Rajagopal', 'Jeffrey Chen', 'Rohan Chandra', 'Zezhou Cheng']
- **Abstrat**: The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.   We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.





## Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation
- **Url**: http://arxiv.org/abs/2512.10925v1
- **Authors**: ['Zamirddine Mari', 'Mohamad Motasem Nawaf', 'Pierre Drap']
- **Abstrat**: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.





## Iterative Compositional Data Generation for Robot Control
- **Url**: http://arxiv.org/abs/2512.10891v1
- **Authors**: ['Anh-Quan Pham', 'Marcel Hussing', 'Shubhankar P. Patankar', 'Dani S. Bassett', 'Jorge Mendez-Mendez', 'Eric Eaton']
- **Abstrat**: Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.




