# soft
## Geometric Confinement Reveals Scale-Free Velocity Correlations in Epithelial Cell Monolayer
- **Url**: http://arxiv.org/abs/2511.21655v1
- **Authors**: ['Guillaume Duprez', 'Mélina Durande', 'François Graner', 'Hélène Delanoë-Ayari']
- **Abstrat**: Collective cell flows are a hallmark of tissue dynamics in development, wound healing, and various diseases. Here, we perform experiments on epithelial MDCK cell monolayers, over tens of hours without jamming, on millimeter-scale micropatterned substrates with or without free front (a strip or a closed racetrack). During maturation in time, domains and long-range correlations of the velocity field appear. Enstrophy increases (along with kinetic energy) during 5 hours, then passes through a maximum and decreases. Spatial velocity correlations are scale-free, following a power law, which challenges the notion of a single intrinsic correlation length. It suggests that the monolayer behaves as a critical-like system where information is transmitted across its entire size, a feature consistent with models of active solids capable of long-range stress propagation. The spatial correlation exponent significantly evolves with time, probably reflecting the monolayer maturation. The size, shape, topology and rigidity of patterned substrate influence the organization of flows and mechanical fields. Spontaneous collective motions are stronger on soft than on hard substrate. The presence of a free front accumulates vimentin on a much larger length scale than a fixed boundary ($65\pm 4$~$μ$m vs $3.2 \pm 0.7$~$μ$m), possibly revealing an underlying polarizing cue on the cell velocity field.





# robot
## TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos
- **Url**: http://arxiv.org/abs/2511.21690v1
- **Authors**: ['Seungjae Lee', 'Yoonkyo Jung', 'Inkook Chun', 'Yao-Chih Lee', 'Zikui Cai', 'Hongjia Huang', 'Aayush Talreja', 'Tan Dat Dao', 'Yongyuan Liang', 'Jia-Bin Huang', 'Furong Huang']
- **Abstrat**: Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.





## Uncertainty Quantification for Visual Object Pose Estimation
- **Url**: http://arxiv.org/abs/2511.21666v1
- **Authors**: ['Lorenzo Shaikewitz', 'Charis Georgiou', 'Luca Carlone']
- **Abstrat**: Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.




