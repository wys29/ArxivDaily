# soft
## Soft and Jet functions for SCET at four loops in QCD
- **Url**: http://arxiv.org/abs/2512.23666v1
- **Authors**: ['Saurav Goyal', 'Sven-Olaf Moch', 'Vaibhav Pathak', 'V. Ravindran']
- **Abstrat**: Soft-Collinear Effective Theory is a framework for systematically organizing and resumming the logarithmic contributions that occur in high-energy reactions. It provides a factorized description of cross sections in terms of hard, jet, soft, and beam functions. As the latter are universal, they can be obtained from the well-known perturbative results in quantum chromodynamics (QCD) for deep-inelastic scattering, Drell-Yan and Higgs boson productions. Using the recent results ~\cite{Kniehl:2025ttz} on four-loop eikonal $(f^I)$ and collinear anomalous dimensions $(B^I)$ for quarks and gluons, $I=q,g$, as well as perturbative results from previous orders, we present four-loop predictions for the quark and gluon soft and jet functions. They constitute an important component of the $N$-jettiness subtraction method at $\rm{N^4LO}$ accuracy in QCD, which eventually may enable the calculation of fully-differential cross sections at higher orders.





# robot
## Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation
- **Url**: http://arxiv.org/abs/2512.23703v1
- **Authors**: ['Huajie Tan', 'Sixiang Chen', 'Yijie Xu', 'Zixiao Wang', 'Yuheng Ji', 'Cheng Chi', 'Yaoxu Lyu', 'Zhongxia Zhao', 'Xiansheng Chen', 'Peterson Co', 'Shaoxuan Xie', 'Guocai Yao', 'Pengwei Wang', 'Zhongyuan Wang', 'Shanghang Zhang']
- **Abstrat**: The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io





## The Bulldozer Technique: Efficient Elimination of Local Minima Traps for APF-Based Robot Navigation
- **Url**: http://arxiv.org/abs/2512.23672v1
- **Authors**: ['Mohammed Baziyad', 'Manal Al Shohna', 'Tamer Rabie']
- **Abstrat**: Path planning is a fundamental component in autonomous mobile robotics, enabling a robot to navigate from its current location to a desired goal while avoiding obstacles. Among the various techniques, Artificial Potential Field (APF) methods have gained popularity due to their simplicity, real-time responsiveness, and low computational requirements. However, a major limitation of conventional APF approaches is the local minima trap problem, where the robot becomes stuck in a position with no clear direction toward the goal. This paper proposes a novel path planning technique, termed the Bulldozer, which addresses the local minima issue while preserving the inherent advantages of APF. The Bulldozer technique introduces a backfilling mechanism that systematically identifies and eliminates local minima regions by increasing their potential values, analogous to a bulldozer filling potholes in a road. Additionally, a ramp-based enhancement is incorporated to assist the robot in escaping trap areas when starting within a local minimum. The proposed technique is experimentally validated using a physical mobile robot across various maps with increasing complexity. Comparative analyses are conducted against standard APF, adaptive APF, and well-established planning algorithms such as A*, PRM, and RRT. Results demonstrate that the Bulldozer technique effectively resolves the local minima problem while achieving superior execution speed and competitive path quality. Furthermore, a kinematic tracking controller is employed to assess the smoothness and traceability of the planned paths, confirming their suitability for real-world execution.




