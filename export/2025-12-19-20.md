# soft
## GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation
- **Url**: http://arxiv.org/abs/2512.16853v1
- **Authors**: ['Amita Kamath', 'Kai-Wei Chang', 'Ranjay Krishna', 'Luke Zettlemoyer', 'Yushi Hu', 'Marjan Ghazvininejad']
- **Abstrat**: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.





## BeppoSAX-WFC catalog of fast X-ray transients
- **Url**: http://arxiv.org/abs/2512.16845v1
- **Authors**: ["J. J. M. in 't Zand", 'C. Guidorzi', 'J. Heise', 'L. Amati', 'E. Kuulkers', 'F. Frontera', 'G. Gianfagna', 'L. Piro']
- **Abstrat**: We performed a search for fast X-ray transients (FXTs), with durations longer than one second and less than one day, through data of the Wide Field Camera (WFC) instrument onboard the BeppoSAX X-ray observatory collected between June 1996 and April 2002. (..) We focused our search on gamma-ray bursts (GRBs), X-ray flashes (XRFs), X-ray flares from high-mass X-ray binaries and stellar flares, while Type-I and II X-ray bursts from Galactic neutron stars were excluded. 149 such fast transient events were detected. 63 of these are new to the literature. 38 flares are identified with 22 nearby stars. Three stars have never been seen flaring before in X-rays or optical (NLTT 51688, GR Dra and UCAC4 255-003783). We find that the MeV transient GRO J1753+57 is most likely the same object as GR Dra rather than an AGN as previously thought. Eleven flares were detected from known high-mass X-ray binaries with irregular wind accretion (four of which are of the subclass of supergiant fast X-ray transients). 100 GRBs were identified of which 24 have not been published before. We classify 37% of the X-ray detected GRBs as XRFs with relatively large X-ray to gamma-ray flux ratio, gamma-rays being measured with the BeppoSAX Gamma Ray Burst Monitor. The duration/spectral hardness distribution of all FXTs is bimodal, separating the group roughly in transients shorter and longer than 1 ksec and with relatively hard and soft spectra, respectively. We identify the 'short' FXTs as GRBs and XRFs and the `long' FXTs as flares from nearby late-type stars and X-ray binaries. The BeppoSAX-WFC FXT sample is found to be consistent with the one observed by Einstein Probe, when the sensitivity of the two instruments is taken into account.





# robot
## Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning
- **Url**: http://arxiv.org/abs/2512.16911v1
- **Authors**: ['Andrew Wagenmaker', 'Perry Dong', 'Raymond Tsao', 'Chelsea Finn', 'Sergey Levine']
- **Abstrat**: Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.





## MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning
- **Url**: http://arxiv.org/abs/2512.16909v1
- **Authors**: ['Yuanchen Ju', 'Yongyuan Liang', 'Yen-Jen Wang', 'Nandiraju Gireesh', 'Yuanliang Ju', 'Seungjae Lee', 'Qiao Gu', 'Elvis Hsieh', 'Furong Huang', 'Koushil Sreenath']
- **Abstrat**: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.





## SceneDiff: A Benchmark and Method for Multiview Object Change Detection
- **Url**: http://arxiv.org/abs/2512.16908v1
- **Authors**: ['Yuqun Wu', 'Chih-hao Lin', 'Henry Che', 'Aditi Tiwari', 'Chuhang Zou', 'Shenlong Wang', 'Derek Hoiem']
- **Abstrat**: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.





## Sceniris: A Fast Procedural Scene Generation Framework
- **Url**: http://arxiv.org/abs/2512.16896v1
- **Authors**: ['Jinghuan Shang', 'Harsh Patel', 'Ran Gong', 'Karl Schmeckpeper']
- **Abstrat**: Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris





## Dual-Channel Tomographic Tactile Skin with Pneumatic Pressure Sensing for Improved Force Estimation
- **Url**: http://arxiv.org/abs/2503.13036v2
- **Authors**: ['Haofeng Chen', 'Jiri Kubik', 'Bedrich Himmel', 'Matej Hoffmann', 'Hyosang Lee']
- **Abstrat**: Tactile skins based on Electrical Impedance Tomography (EIT) enable large-area contact localization with few electrodes, but suffer from nonuniform sensitivity that limits force estimation accuracy. This work introduces a dual-channel tactile skin that integrates an EIT layer with a pneumatic pressure layer and a calibration framework that leverages their complementary strengths. The EIT layer provides robust multi-contact localization, while the pneumatic pressure layer supplies a stable scalar measurement that serves as contact force estimation. A location-aware correction method is introduced, learning smooth spatial gain and offset fields from a single-session calibration, enabling spatially consistent multi-contact force estimation. The proposed system achieves accurate force estimation across diverse contact configurations, generalizes to varying indenter sizes, and preserves EIT's inherent advantages in multi-contact localization. By letting the pneumatic pressure layer handle the force estimation and using the EIT layer to determine where each contact occurs, the method avoids the need for large datasets, complicated calibration setups, and heavy machine-learning pipelines often required by previous EIT-only approaches. This dual-channel design provides a practical, scalable, and easy-to-calibrate solution for building large-area robotic skins.





## PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies
- **Url**: http://arxiv.org/abs/2512.16881v1
- **Authors**: ['Arhan Jain', 'Mingtong Zhang', 'Kanav Arora', 'William Chen', 'Marcel Torne', 'Muhammad Zubair Irshad', 'Sergey Zakharov', 'Yue Wang', 'Sergey Levine', 'Chelsea Finn', 'Wei-Chiu Ma', 'Dhruv Shah', 'Abhishek Gupta', 'Karl Pertsch']
- **Abstrat**: A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.





## ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning
- **Url**: http://arxiv.org/abs/2512.16861v1
- **Authors**: ['Zihan Zhou', 'Animesh Garg', 'Ajay Mandlekar', 'Caelan Garrett']
- **Abstrat**: Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/





## OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction
- **Url**: http://arxiv.org/abs/2512.16842v1
- **Authors**: ['Yuxin Ray Song', 'Jinzhou Li', 'Rao Fu', 'Devin Murphy', 'Kaichen Zhou', 'Rishi Shiv', 'Yaqi Li', 'Haoyu Xiong', 'Crystal Elaine Owens', 'Yilun Du', 'Yiyue Luo', 'Xianyi Cheng', 'Antonio Torralba', 'Wojciech Matusik', 'Paul Pu Liang']
- **Abstrat**: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.




