# soft
# robot
## Mixture of Horizons in Action Chunking
- **Url**: http://arxiv.org/abs/2511.19433v1
- **Authors**: ['Dong Jing', 'Gang Wang', 'Jiaqi Liu', 'Weiliang Tang', 'Zelong Sun', 'Yunchao Yao', 'Zhenyu Wei', 'Yunhui Liu', 'Zhiwu Lu', 'Mingyu Ding']
- **Abstrat**: Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\textbf{action chunk length}$ used during training, termed $\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons





## Robotic chip-scale nanofabrication for superior consistency
- **Url**: http://arxiv.org/abs/2511.19432v1
- **Authors**: ['Felix M. Mayor', 'Wenyan Guan', 'Erik Szakiel', 'Amir H. Safavi-Naeini', 'Samuel Gyger']
- **Abstrat**: Unlike the rigid, high-volume automation found in industry, academic research requires process flexibility that has historically relied on variable manual operations. This hinders the fabrication of advanced, complex devices. We propose to address this gap by automating these low-volume, high-stakes tasks using a robotic arm to improve process control and consistency. As a proof of concept, we deploy this system for the resist development of Josephson junction devices. A statistical comparison of the process repeatability shows the robotic process achieves a resistance spread across chips close to 2%, a significant improvement over the ~7% spread observed from human operators, validating robotics as a solution to eliminate operator-dependent variability and a path towards industrial-level consistency in a research setting.





## Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments
- **Url**: http://arxiv.org/abs/2511.19396v1
- **Authors**: ['Jorge Ortigoso-Narro', 'Jose A. Belloch', 'Adrian Amor-Martin', 'Sandra Roger', 'Maximo Cobos']
- **Abstrat**: Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.





## Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics
- **Url**: http://arxiv.org/abs/2501.10100v4
- **Authors**: ['Chenhao Li', 'Andreas Krause', 'Marco Hutter']
- **Abstrat**: Learning robust and generalizable world models is crucial for enabling efficient and scalable robotic control in real-world environments. In this work, we introduce a novel framework for learning world models that accurately capture complex, partially observable, and stochastic dynamics. The proposed method employs a dual-autoregressive mechanism and self-supervised training to achieve reliable long-horizon predictions without relying on domain-specific inductive biases, ensuring adaptability across diverse robotic tasks. We further propose a policy optimization framework that leverages world models for efficient training in imagined environments and seamless deployment in real-world systems. This work advances model-based reinforcement learning by addressing the challenges of long-horizon prediction, error accumulation, and sim-to-real transfer. By providing a scalable and robust framework, the introduced methods pave the way for adaptive and efficient robotic systems in real-world applications.




